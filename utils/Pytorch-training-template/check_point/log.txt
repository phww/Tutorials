2021-06-16::22-49-01
=============== args ===============
epochs:20
batch_size:50
init_lr:0.0001
=============== template config ===============
optimizer_list:[Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)]
criterion:CrossEntropyLoss()
log_per_step:100
global_step:0
global_step_eval:0
epoch:1
lr_scheduler_list:[<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1b8a42ee0>]
lr_scheduler_type:metric
device:cuda
ckpt_dir:./check_point
=============== model info ===============
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [32, 64, 32, 32]           9,408
       BatchNorm2d-2           [32, 64, 32, 32]             128
              ReLU-3           [32, 64, 32, 32]               0
         MaxPool2d-4           [32, 64, 16, 16]               0
            Conv2d-5           [32, 64, 16, 16]           4,096
       BatchNorm2d-6           [32, 64, 16, 16]             128
              ReLU-7           [32, 64, 16, 16]               0
            Conv2d-8           [32, 64, 16, 16]          36,864
       BatchNorm2d-9           [32, 64, 16, 16]             128
             ReLU-10           [32, 64, 16, 16]               0
           Conv2d-11          [32, 256, 16, 16]          16,384
      BatchNorm2d-12          [32, 256, 16, 16]             512
           Conv2d-13          [32, 256, 16, 16]          16,384
      BatchNorm2d-14          [32, 256, 16, 16]             512
             ReLU-15          [32, 256, 16, 16]               0
       Bottleneck-16          [32, 256, 16, 16]               0
           Conv2d-17           [32, 64, 16, 16]          16,384
      BatchNorm2d-18           [32, 64, 16, 16]             128
             ReLU-19           [32, 64, 16, 16]               0
           Conv2d-20           [32, 64, 16, 16]          36,864
      BatchNorm2d-21           [32, 64, 16, 16]             128
             ReLU-22           [32, 64, 16, 16]               0
           Conv2d-23          [32, 256, 16, 16]          16,384
      BatchNorm2d-24          [32, 256, 16, 16]             512
             ReLU-25          [32, 256, 16, 16]               0
       Bottleneck-26          [32, 256, 16, 16]               0
           Conv2d-27           [32, 64, 16, 16]          16,384
      BatchNorm2d-28           [32, 64, 16, 16]             128
             ReLU-29           [32, 64, 16, 16]               0
           Conv2d-30           [32, 64, 16, 16]          36,864
      BatchNorm2d-31           [32, 64, 16, 16]             128
             ReLU-32           [32, 64, 16, 16]               0
           Conv2d-33          [32, 256, 16, 16]          16,384
      BatchNorm2d-34          [32, 256, 16, 16]             512
             ReLU-35          [32, 256, 16, 16]               0
       Bottleneck-36          [32, 256, 16, 16]               0
           Conv2d-37          [32, 128, 16, 16]          32,768
      BatchNorm2d-38          [32, 128, 16, 16]             256
             ReLU-39          [32, 128, 16, 16]               0
           Conv2d-40            [32, 128, 8, 8]         147,456
      BatchNorm2d-41            [32, 128, 8, 8]             256
             ReLU-42            [32, 128, 8, 8]               0
           Conv2d-43            [32, 512, 8, 8]          65,536
      BatchNorm2d-44            [32, 512, 8, 8]           1,024
           Conv2d-45            [32, 512, 8, 8]         131,072
      BatchNorm2d-46            [32, 512, 8, 8]           1,024
             ReLU-47            [32, 512, 8, 8]               0
       Bottleneck-48            [32, 512, 8, 8]               0
           Conv2d-49            [32, 128, 8, 8]          65,536
      BatchNorm2d-50            [32, 128, 8, 8]             256
             ReLU-51            [32, 128, 8, 8]               0
           Conv2d-52            [32, 128, 8, 8]         147,456
      BatchNorm2d-53            [32, 128, 8, 8]             256
             ReLU-54            [32, 128, 8, 8]               0
           Conv2d-55            [32, 512, 8, 8]          65,536
      BatchNorm2d-56            [32, 512, 8, 8]           1,024
             ReLU-57            [32, 512, 8, 8]               0
       Bottleneck-58            [32, 512, 8, 8]               0
           Conv2d-59            [32, 128, 8, 8]          65,536
      BatchNorm2d-60            [32, 128, 8, 8]             256
             ReLU-61            [32, 128, 8, 8]               0
           Conv2d-62            [32, 128, 8, 8]         147,456
      BatchNorm2d-63            [32, 128, 8, 8]             256
             ReLU-64            [32, 128, 8, 8]               0
           Conv2d-65            [32, 512, 8, 8]          65,536
      BatchNorm2d-66            [32, 512, 8, 8]           1,024
             ReLU-67            [32, 512, 8, 8]               0
       Bottleneck-68            [32, 512, 8, 8]               0
           Conv2d-69            [32, 128, 8, 8]          65,536
      BatchNorm2d-70            [32, 128, 8, 8]             256
             ReLU-71            [32, 128, 8, 8]               0
           Conv2d-72            [32, 128, 8, 8]         147,456
      BatchNorm2d-73            [32, 128, 8, 8]             256
             ReLU-74            [32, 128, 8, 8]               0
           Conv2d-75            [32, 512, 8, 8]          65,536
      BatchNorm2d-76            [32, 512, 8, 8]           1,024
             ReLU-77            [32, 512, 8, 8]               0
       Bottleneck-78            [32, 512, 8, 8]               0
           Conv2d-79            [32, 256, 8, 8]         131,072
      BatchNorm2d-80            [32, 256, 8, 8]             512
             ReLU-81            [32, 256, 8, 8]               0
           Conv2d-82            [32, 256, 4, 4]         589,824
      BatchNorm2d-83            [32, 256, 4, 4]             512
             ReLU-84            [32, 256, 4, 4]               0
           Conv2d-85           [32, 1024, 4, 4]         262,144
      BatchNorm2d-86           [32, 1024, 4, 4]           2,048
           Conv2d-87           [32, 1024, 4, 4]         524,288
      BatchNorm2d-88           [32, 1024, 4, 4]           2,048
             ReLU-89           [32, 1024, 4, 4]               0
       Bottleneck-90           [32, 1024, 4, 4]               0
           Conv2d-91            [32, 256, 4, 4]         262,144
      BatchNorm2d-92            [32, 256, 4, 4]             512
             ReLU-93            [32, 256, 4, 4]               0
           Conv2d-94            [32, 256, 4, 4]         589,824
      BatchNorm2d-95            [32, 256, 4, 4]             512
             ReLU-96            [32, 256, 4, 4]               0
           Conv2d-97           [32, 1024, 4, 4]         262,144
      BatchNorm2d-98           [32, 1024, 4, 4]           2,048
             ReLU-99           [32, 1024, 4, 4]               0
      Bottleneck-100           [32, 1024, 4, 4]               0
          Conv2d-101            [32, 256, 4, 4]         262,144
     BatchNorm2d-102            [32, 256, 4, 4]             512
            ReLU-103            [32, 256, 4, 4]               0
          Conv2d-104            [32, 256, 4, 4]         589,824
     BatchNorm2d-105            [32, 256, 4, 4]             512
            ReLU-106            [32, 256, 4, 4]               0
          Conv2d-107           [32, 1024, 4, 4]         262,144
     BatchNorm2d-108           [32, 1024, 4, 4]           2,048
            ReLU-109           [32, 1024, 4, 4]               0
      Bottleneck-110           [32, 1024, 4, 4]               0
          Conv2d-111            [32, 256, 4, 4]         262,144
     BatchNorm2d-112            [32, 256, 4, 4]             512
            ReLU-113            [32, 256, 4, 4]               0
          Conv2d-114            [32, 256, 4, 4]         589,824
     BatchNorm2d-115            [32, 256, 4, 4]             512
            ReLU-116            [32, 256, 4, 4]               0
          Conv2d-117           [32, 1024, 4, 4]         262,144
     BatchNorm2d-118           [32, 1024, 4, 4]           2,048
            ReLU-119           [32, 1024, 4, 4]               0
      Bottleneck-120           [32, 1024, 4, 4]               0
          Conv2d-121            [32, 256, 4, 4]         262,144
     BatchNorm2d-122            [32, 256, 4, 4]             512
            ReLU-123            [32, 256, 4, 4]               0
          Conv2d-124            [32, 256, 4, 4]         589,824
     BatchNorm2d-125            [32, 256, 4, 4]             512
            ReLU-126            [32, 256, 4, 4]               0
          Conv2d-127           [32, 1024, 4, 4]         262,144
     BatchNorm2d-128           [32, 1024, 4, 4]           2,048
            ReLU-129           [32, 1024, 4, 4]               0
      Bottleneck-130           [32, 1024, 4, 4]               0
          Conv2d-131            [32, 256, 4, 4]         262,144
     BatchNorm2d-132            [32, 256, 4, 4]             512
            ReLU-133            [32, 256, 4, 4]               0
          Conv2d-134            [32, 256, 4, 4]         589,824
     BatchNorm2d-135            [32, 256, 4, 4]             512
            ReLU-136            [32, 256, 4, 4]               0
          Conv2d-137           [32, 1024, 4, 4]         262,144
     BatchNorm2d-138           [32, 1024, 4, 4]           2,048
            ReLU-139           [32, 1024, 4, 4]               0
      Bottleneck-140           [32, 1024, 4, 4]               0
          Conv2d-141            [32, 256, 4, 4]         262,144
     BatchNorm2d-142            [32, 256, 4, 4]             512
            ReLU-143            [32, 256, 4, 4]               0
          Conv2d-144            [32, 256, 4, 4]         589,824
     BatchNorm2d-145            [32, 256, 4, 4]             512
            ReLU-146            [32, 256, 4, 4]               0
          Conv2d-147           [32, 1024, 4, 4]         262,144
     BatchNorm2d-148           [32, 1024, 4, 4]           2,048
            ReLU-149           [32, 1024, 4, 4]               0
      Bottleneck-150           [32, 1024, 4, 4]               0
          Conv2d-151            [32, 256, 4, 4]         262,144
     BatchNorm2d-152            [32, 256, 4, 4]             512
            ReLU-153            [32, 256, 4, 4]               0
          Conv2d-154            [32, 256, 4, 4]         589,824
     BatchNorm2d-155            [32, 256, 4, 4]             512
            ReLU-156            [32, 256, 4, 4]               0
          Conv2d-157           [32, 1024, 4, 4]         262,144
     BatchNorm2d-158           [32, 1024, 4, 4]           2,048
            ReLU-159           [32, 1024, 4, 4]               0
      Bottleneck-160           [32, 1024, 4, 4]               0
          Conv2d-161            [32, 256, 4, 4]         262,144
     BatchNorm2d-162            [32, 256, 4, 4]             512
            ReLU-163            [32, 256, 4, 4]               0
          Conv2d-164            [32, 256, 4, 4]         589,824
     BatchNorm2d-165            [32, 256, 4, 4]             512
            ReLU-166            [32, 256, 4, 4]               0
          Conv2d-167           [32, 1024, 4, 4]         262,144
     BatchNorm2d-168           [32, 1024, 4, 4]           2,048
            ReLU-169           [32, 1024, 4, 4]               0
      Bottleneck-170           [32, 1024, 4, 4]               0
          Conv2d-171            [32, 256, 4, 4]         262,144
     BatchNorm2d-172            [32, 256, 4, 4]             512
            ReLU-173            [32, 256, 4, 4]               0
          Conv2d-174            [32, 256, 4, 4]         589,824
     BatchNorm2d-175            [32, 256, 4, 4]             512
            ReLU-176            [32, 256, 4, 4]               0
          Conv2d-177           [32, 1024, 4, 4]         262,144
     BatchNorm2d-178           [32, 1024, 4, 4]           2,048
            ReLU-179           [32, 1024, 4, 4]               0
      Bottleneck-180           [32, 1024, 4, 4]               0
          Conv2d-181            [32, 256, 4, 4]         262,144
     BatchNorm2d-182            [32, 256, 4, 4]             512
            ReLU-183            [32, 256, 4, 4]               0
          Conv2d-184            [32, 256, 4, 4]         589,824
     BatchNorm2d-185            [32, 256, 4, 4]             512
            ReLU-186            [32, 256, 4, 4]               0
          Conv2d-187           [32, 1024, 4, 4]         262,144
     BatchNorm2d-188           [32, 1024, 4, 4]           2,048
            ReLU-189           [32, 1024, 4, 4]               0
      Bottleneck-190           [32, 1024, 4, 4]               0
          Conv2d-191            [32, 256, 4, 4]         262,144
     BatchNorm2d-192            [32, 256, 4, 4]             512
            ReLU-193            [32, 256, 4, 4]               0
          Conv2d-194            [32, 256, 4, 4]         589,824
     BatchNorm2d-195            [32, 256, 4, 4]             512
            ReLU-196            [32, 256, 4, 4]               0
          Conv2d-197           [32, 1024, 4, 4]         262,144
     BatchNorm2d-198           [32, 1024, 4, 4]           2,048
            ReLU-199           [32, 1024, 4, 4]               0
      Bottleneck-200           [32, 1024, 4, 4]               0
          Conv2d-201            [32, 256, 4, 4]         262,144
     BatchNorm2d-202            [32, 256, 4, 4]             512
            ReLU-203            [32, 256, 4, 4]               0
          Conv2d-204            [32, 256, 4, 4]         589,824
     BatchNorm2d-205            [32, 256, 4, 4]             512
            ReLU-206            [32, 256, 4, 4]               0
          Conv2d-207           [32, 1024, 4, 4]         262,144
     BatchNorm2d-208           [32, 1024, 4, 4]           2,048
            ReLU-209           [32, 1024, 4, 4]               0
      Bottleneck-210           [32, 1024, 4, 4]               0
          Conv2d-211            [32, 256, 4, 4]         262,144
     BatchNorm2d-212            [32, 256, 4, 4]             512
            ReLU-213            [32, 256, 4, 4]               0
          Conv2d-214            [32, 256, 4, 4]         589,824
     BatchNorm2d-215            [32, 256, 4, 4]             512
            ReLU-216            [32, 256, 4, 4]               0
          Conv2d-217           [32, 1024, 4, 4]         262,144
     BatchNorm2d-218           [32, 1024, 4, 4]           2,048
            ReLU-219           [32, 1024, 4, 4]               0
      Bottleneck-220           [32, 1024, 4, 4]               0
          Conv2d-221            [32, 256, 4, 4]         262,144
     BatchNorm2d-222            [32, 256, 4, 4]             512
            ReLU-223            [32, 256, 4, 4]               0
          Conv2d-224            [32, 256, 4, 4]         589,824
     BatchNorm2d-225            [32, 256, 4, 4]             512
            ReLU-226            [32, 256, 4, 4]               0
          Conv2d-227           [32, 1024, 4, 4]         262,144
     BatchNorm2d-228           [32, 1024, 4, 4]           2,048
            ReLU-229           [32, 1024, 4, 4]               0
      Bottleneck-230           [32, 1024, 4, 4]               0
          Conv2d-231            [32, 256, 4, 4]         262,144
     BatchNorm2d-232            [32, 256, 4, 4]             512
            ReLU-233            [32, 256, 4, 4]               0
          Conv2d-234            [32, 256, 4, 4]         589,824
     BatchNorm2d-235            [32, 256, 4, 4]             512
            ReLU-236            [32, 256, 4, 4]               0
          Conv2d-237           [32, 1024, 4, 4]         262,144
     BatchNorm2d-238           [32, 1024, 4, 4]           2,048
            ReLU-239           [32, 1024, 4, 4]               0
      Bottleneck-240           [32, 1024, 4, 4]               0
          Conv2d-241            [32, 256, 4, 4]         262,144
     BatchNorm2d-242            [32, 256, 4, 4]             512
            ReLU-243            [32, 256, 4, 4]               0
          Conv2d-244            [32, 256, 4, 4]         589,824
     BatchNorm2d-245            [32, 256, 4, 4]             512
            ReLU-246            [32, 256, 4, 4]               0
          Conv2d-247           [32, 1024, 4, 4]         262,144
     BatchNorm2d-248           [32, 1024, 4, 4]           2,048
            ReLU-249           [32, 1024, 4, 4]               0
      Bottleneck-250           [32, 1024, 4, 4]               0
          Conv2d-251            [32, 256, 4, 4]         262,144
     BatchNorm2d-252            [32, 256, 4, 4]             512
            ReLU-253            [32, 256, 4, 4]               0
          Conv2d-254            [32, 256, 4, 4]         589,824
     BatchNorm2d-255            [32, 256, 4, 4]             512
            ReLU-256            [32, 256, 4, 4]               0
          Conv2d-257           [32, 1024, 4, 4]         262,144
     BatchNorm2d-258           [32, 1024, 4, 4]           2,048
            ReLU-259           [32, 1024, 4, 4]               0
      Bottleneck-260           [32, 1024, 4, 4]               0
          Conv2d-261            [32, 256, 4, 4]         262,144
     BatchNorm2d-262            [32, 256, 4, 4]             512
            ReLU-263            [32, 256, 4, 4]               0
          Conv2d-264            [32, 256, 4, 4]         589,824
     BatchNorm2d-265            [32, 256, 4, 4]             512
            ReLU-266            [32, 256, 4, 4]               0
          Conv2d-267           [32, 1024, 4, 4]         262,144
     BatchNorm2d-268           [32, 1024, 4, 4]           2,048
            ReLU-269           [32, 1024, 4, 4]               0
      Bottleneck-270           [32, 1024, 4, 4]               0
          Conv2d-271            [32, 256, 4, 4]         262,144
     BatchNorm2d-272            [32, 256, 4, 4]             512
            ReLU-273            [32, 256, 4, 4]               0
          Conv2d-274            [32, 256, 4, 4]         589,824
     BatchNorm2d-275            [32, 256, 4, 4]             512
            ReLU-276            [32, 256, 4, 4]               0
          Conv2d-277           [32, 1024, 4, 4]         262,144
     BatchNorm2d-278           [32, 1024, 4, 4]           2,048
            ReLU-279           [32, 1024, 4, 4]               0
      Bottleneck-280           [32, 1024, 4, 4]               0
          Conv2d-281            [32, 256, 4, 4]         262,144
     BatchNorm2d-282            [32, 256, 4, 4]             512
            ReLU-283            [32, 256, 4, 4]               0
          Conv2d-284            [32, 256, 4, 4]         589,824
     BatchNorm2d-285            [32, 256, 4, 4]             512
            ReLU-286            [32, 256, 4, 4]               0
          Conv2d-287           [32, 1024, 4, 4]         262,144
     BatchNorm2d-288           [32, 1024, 4, 4]           2,048
            ReLU-289           [32, 1024, 4, 4]               0
      Bottleneck-290           [32, 1024, 4, 4]               0
          Conv2d-291            [32, 256, 4, 4]         262,144
     BatchNorm2d-292            [32, 256, 4, 4]             512
            ReLU-293            [32, 256, 4, 4]               0
          Conv2d-294            [32, 256, 4, 4]         589,824
     BatchNorm2d-295            [32, 256, 4, 4]             512
            ReLU-296            [32, 256, 4, 4]               0
          Conv2d-297           [32, 1024, 4, 4]         262,144
     BatchNorm2d-298           [32, 1024, 4, 4]           2,048
            ReLU-299           [32, 1024, 4, 4]               0
      Bottleneck-300           [32, 1024, 4, 4]               0
          Conv2d-301            [32, 256, 4, 4]         262,144
     BatchNorm2d-302            [32, 256, 4, 4]             512
            ReLU-303            [32, 256, 4, 4]               0
          Conv2d-304            [32, 256, 4, 4]         589,824
     BatchNorm2d-305            [32, 256, 4, 4]             512
            ReLU-306            [32, 256, 4, 4]               0
          Conv2d-307           [32, 1024, 4, 4]         262,144
     BatchNorm2d-308           [32, 1024, 4, 4]           2,048
            ReLU-309           [32, 1024, 4, 4]               0
      Bottleneck-310           [32, 1024, 4, 4]               0
          Conv2d-311            [32, 512, 4, 4]         524,288
     BatchNorm2d-312            [32, 512, 4, 4]           1,024
            ReLU-313            [32, 512, 4, 4]               0
          Conv2d-314            [32, 512, 2, 2]       2,359,296
     BatchNorm2d-315            [32, 512, 2, 2]           1,024
            ReLU-316            [32, 512, 2, 2]               0
          Conv2d-317           [32, 2048, 2, 2]       1,048,576
     BatchNorm2d-318           [32, 2048, 2, 2]           4,096
          Conv2d-319           [32, 2048, 2, 2]       2,097,152
     BatchNorm2d-320           [32, 2048, 2, 2]           4,096
            ReLU-321           [32, 2048, 2, 2]               0
      Bottleneck-322           [32, 2048, 2, 2]               0
          Conv2d-323            [32, 512, 2, 2]       1,048,576
     BatchNorm2d-324            [32, 512, 2, 2]           1,024
            ReLU-325            [32, 512, 2, 2]               0
          Conv2d-326            [32, 512, 2, 2]       2,359,296
     BatchNorm2d-327            [32, 512, 2, 2]           1,024
            ReLU-328            [32, 512, 2, 2]               0
          Conv2d-329           [32, 2048, 2, 2]       1,048,576
     BatchNorm2d-330           [32, 2048, 2, 2]           4,096
            ReLU-331           [32, 2048, 2, 2]               0
      Bottleneck-332           [32, 2048, 2, 2]               0
          Conv2d-333            [32, 512, 2, 2]       1,048,576
     BatchNorm2d-334            [32, 512, 2, 2]           1,024
            ReLU-335            [32, 512, 2, 2]               0
          Conv2d-336            [32, 512, 2, 2]       2,359,296
     BatchNorm2d-337            [32, 512, 2, 2]           1,024
            ReLU-338            [32, 512, 2, 2]               0
          Conv2d-339           [32, 2048, 2, 2]       1,048,576
     BatchNorm2d-340           [32, 2048, 2, 2]           4,096
            ReLU-341           [32, 2048, 2, 2]               0
      Bottleneck-342           [32, 2048, 2, 2]               0
AdaptiveAvgPool2d-343           [32, 2048, 1, 1]               0
          Linear-344                 [32, 1000]       2,049,000
================================================================
Total params: 44,549,160
Trainable params: 44,549,160
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 1.50
Forward/backward pass size (MB): 1123.24
Params size (MB): 169.94
Estimated Total Size (MB): 1294.69
----------------------------------------------------------------
None
*************** epoch:1 ***************
loss: 7.28265	cur:[0]\[50000]
loss: 1.40542	cur:[5000]\[50000]
loss: 1.18921	cur:[10000]\[50000]
loss: 1.17611	cur:[15000]\[50000]
loss: 1.15475	cur:[20000]\[50000]
loss: 1.13848	cur:[25000]\[50000]
loss: 1.11848	cur:[30000]\[50000]
loss: 1.11864	cur:[35000]\[50000]
loss: 1.09886	cur:[40000]\[50000]
loss: 1.07840	cur:[45000]\[50000]
loss: 1.07162	cur:[50000]\[50000]
epoch:1	avg_epoch_loss2.30999
--------------- Evaluation ---------------
loss: 1.06235	cur:[5000]\[10000]
loss: 1.05565	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:1	avg_running_loss:2.11800
epoch:1	acc:0.22270
*************** epoch:2 ***************
loss: 1.06444	cur:[5000]\[50000]
loss: 1.05261	cur:[10000]\[50000]
loss: 1.03930	cur:[15000]\[50000]
loss: 1.03595	cur:[20000]\[50000]
loss: 1.02751	cur:[25000]\[50000]
loss: 1.01656	cur:[30000]\[50000]
loss: 1.01405	cur:[35000]\[50000]
loss: 1.01330	cur:[40000]\[50000]
loss: 1.00486	cur:[45000]\[50000]
loss: 0.99221	cur:[50000]\[50000]
epoch:2	avg_epoch_loss2.05216
--------------- Evaluation ---------------
loss: 0.99984	cur:[5000]\[10000]
loss: 0.99095	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:2	avg_running_loss:1.99078
epoch:2	acc:0.27000
*************** epoch:3 ***************
loss: 0.97608	cur:[5000]\[50000]
loss: 0.98073	cur:[10000]\[50000]
loss: 0.98092	cur:[15000]\[50000]
loss: 0.97012	cur:[20000]\[50000]
loss: 0.95298	cur:[25000]\[50000]
loss: 0.96512	cur:[30000]\[50000]
loss: 0.94303	cur:[35000]\[50000]
loss: 0.95104	cur:[40000]\[50000]
loss: 0.94543	cur:[45000]\[50000]
loss: 0.94370	cur:[50000]\[50000]
epoch:3	avg_epoch_loss1.92183
--------------- Evaluation ---------------
loss: 0.91475	cur:[5000]\[10000]
loss: 0.91077	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:3	avg_running_loss:1.82553
epoch:3	acc:0.33690
*************** epoch:4 ***************
loss: 0.91933	cur:[5000]\[50000]
loss: 0.90687	cur:[10000]\[50000]
loss: 0.92292	cur:[15000]\[50000]
loss: 0.92343	cur:[20000]\[50000]
loss: 0.90368	cur:[25000]\[50000]
loss: 0.90530	cur:[30000]\[50000]
loss: 0.90882	cur:[35000]\[50000]
loss: 0.90116	cur:[40000]\[50000]
loss: 0.89389	cur:[45000]\[50000]
loss: 0.88427	cur:[50000]\[50000]
epoch:4	avg_epoch_loss1.81393
--------------- Evaluation ---------------
loss: 0.87509	cur:[5000]\[10000]
loss: 0.85828	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:4	avg_running_loss:1.73337
epoch:4	acc:0.37790
*************** epoch:5 ***************
loss: 0.88777	cur:[5000]\[50000]
loss: 0.87698	cur:[10000]\[50000]
loss: 0.86451	cur:[15000]\[50000]
loss: 0.85537	cur:[20000]\[50000]
loss: 0.87856	cur:[25000]\[50000]
loss: 0.86326	cur:[30000]\[50000]
loss: 0.86067	cur:[35000]\[50000]
loss: 0.86502	cur:[40000]\[50000]
loss: 0.85174	cur:[45000]\[50000]
loss: 0.84488	cur:[50000]\[50000]
epoch:5	avg_epoch_loss1.72975
--------------- Evaluation ---------------
loss: 0.90707	cur:[5000]\[10000]
loss: 0.90766	cur:[10000]\[10000]
save model at ./check_point/best.pth
save model at ./check_point/epoch5.pth
epoch:5	avg_running_loss:1.81473
epoch:5	acc:0.38430
*************** epoch:6 ***************
loss: 0.83865	cur:[5000]\[50000]
loss: 0.83983	cur:[10000]\[50000]
loss: 0.84395	cur:[15000]\[50000]
loss: 0.84166	cur:[20000]\[50000]
loss: 0.82289	cur:[25000]\[50000]
loss: 0.82012	cur:[30000]\[50000]
loss: 0.83303	cur:[35000]\[50000]
loss: 0.82573	cur:[40000]\[50000]
loss: 0.81560	cur:[45000]\[50000]
loss: 0.80472	cur:[50000]\[50000]
epoch:6	avg_epoch_loss1.65724
--------------- Evaluation ---------------
loss: 0.81747	cur:[5000]\[10000]
loss: 0.81332	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:6	avg_running_loss:1.63079
epoch:6	acc:0.42140
*************** epoch:7 ***************
loss: 0.82518	cur:[5000]\[50000]
loss: 0.80463	cur:[10000]\[50000]
loss: 0.81397	cur:[15000]\[50000]
loss: 0.79208	cur:[20000]\[50000]
loss: 0.79632	cur:[25000]\[50000]
loss: 0.78914	cur:[30000]\[50000]
loss: 0.79130	cur:[35000]\[50000]
loss: 0.80771	cur:[40000]\[50000]
loss: 0.78927	cur:[45000]\[50000]
loss: 0.78117	cur:[50000]\[50000]
epoch:7	avg_epoch_loss1.59816
--------------- Evaluation ---------------
loss: 0.76070	cur:[5000]\[10000]
loss: 0.77163	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:7	avg_running_loss:1.53233
epoch:7	acc:0.45260
*************** epoch:8 ***************
loss: 0.77993	cur:[5000]\[50000]
loss: 0.77488	cur:[10000]\[50000]
loss: 0.77734	cur:[15000]\[50000]
loss: 0.76331	cur:[20000]\[50000]
loss: 0.75919	cur:[25000]\[50000]
loss: 0.77293	cur:[30000]\[50000]
loss: 0.75014	cur:[35000]\[50000]
loss: 0.77254	cur:[40000]\[50000]
loss: 0.76840	cur:[45000]\[50000]
loss: 0.77107	cur:[50000]\[50000]
epoch:8	avg_epoch_loss1.53794
--------------- Evaluation ---------------
loss: 0.75643	cur:[5000]\[10000]
loss: 0.75621	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:8	avg_running_loss:1.51264
epoch:8	acc:0.44690
*************** epoch:9 ***************
loss: 0.75930	cur:[5000]\[50000]
loss: 0.74808	cur:[10000]\[50000]
loss: 0.74558	cur:[15000]\[50000]
loss: 0.74709	cur:[20000]\[50000]
loss: 0.73932	cur:[25000]\[50000]
loss: 0.74763	cur:[30000]\[50000]
loss: 0.74067	cur:[35000]\[50000]
loss: 0.73021	cur:[40000]\[50000]
loss: 0.74846	cur:[45000]\[50000]
loss: 0.73061	cur:[50000]\[50000]
epoch:9	avg_epoch_loss1.48739
--------------- Evaluation ---------------
loss: 0.72233	cur:[5000]\[10000]
loss: 0.73304	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:9	avg_running_loss:1.45537
epoch:9	acc:0.48070
*************** epoch:10 ***************
loss: 0.71924	cur:[5000]\[50000]
loss: 0.72442	cur:[10000]\[50000]
loss: 0.72435	cur:[15000]\[50000]
loss: 0.72843	cur:[20000]\[50000]
loss: 0.71934	cur:[25000]\[50000]
loss: 0.70926	cur:[30000]\[50000]
loss: 0.72744	cur:[35000]\[50000]
loss: 0.72097	cur:[40000]\[50000]
loss: 0.70815	cur:[45000]\[50000]
loss: 0.70548	cur:[50000]\[50000]
epoch:10	avg_epoch_loss1.43742
--------------- Evaluation ---------------
loss: 0.72152	cur:[5000]\[10000]
loss: 0.71845	cur:[10000]\[10000]
save model at ./check_point/best.pth
save model at ./check_point/epoch10.pth
epoch:10	avg_running_loss:1.43997
epoch:10	acc:0.48750
*************** epoch:11 ***************
loss: 0.69293	cur:[5000]\[50000]
loss: 0.70617	cur:[10000]\[50000]
loss: 0.69864	cur:[15000]\[50000]
loss: 0.72876	cur:[20000]\[50000]
loss: 0.71037	cur:[25000]\[50000]
loss: 0.69249	cur:[30000]\[50000]
loss: 0.69261	cur:[35000]\[50000]
loss: 0.69029	cur:[40000]\[50000]
loss: 0.68612	cur:[45000]\[50000]
loss: 0.69816	cur:[50000]\[50000]
epoch:11	avg_epoch_loss1.39931
--------------- Evaluation ---------------
loss: 0.74454	cur:[5000]\[10000]
loss: 0.73866	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:11	avg_running_loss:1.48321
epoch:11	acc:0.47820
*************** epoch:12 ***************
loss: 0.66556	cur:[5000]\[50000]
loss: 0.68955	cur:[10000]\[50000]
loss: 0.66811	cur:[15000]\[50000]
loss: 0.68451	cur:[20000]\[50000]
loss: 0.68534	cur:[25000]\[50000]
loss: 0.67376	cur:[30000]\[50000]
loss: 0.68850	cur:[35000]\[50000]
loss: 0.68051	cur:[40000]\[50000]
loss: 0.67421	cur:[45000]\[50000]
loss: 0.69531	cur:[50000]\[50000]
epoch:12	avg_epoch_loss1.36107
--------------- Evaluation ---------------
loss: 0.71395	cur:[5000]\[10000]
loss: 0.71814	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:12	avg_running_loss:1.43208
epoch:12	acc:0.51120
*************** epoch:13 ***************
loss: 0.67308	cur:[5000]\[50000]
loss: 0.66741	cur:[10000]\[50000]
loss: 0.66969	cur:[15000]\[50000]
loss: 0.65869	cur:[20000]\[50000]
loss: 0.66356	cur:[25000]\[50000]
loss: 0.65415	cur:[30000]\[50000]
loss: 0.66924	cur:[35000]\[50000]
loss: 0.65107	cur:[40000]\[50000]
loss: 0.65752	cur:[45000]\[50000]
loss: 0.64748	cur:[50000]\[50000]
epoch:13	avg_epoch_loss1.32238
--------------- Evaluation ---------------
loss: 0.67575	cur:[5000]\[10000]
loss: 0.66138	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:13	avg_running_loss:1.33712
epoch:13	acc:0.52390
*************** epoch:14 ***************
loss: 0.64500	cur:[5000]\[50000]
loss: 0.63947	cur:[10000]\[50000]
loss: 0.62793	cur:[15000]\[50000]
loss: 0.63976	cur:[20000]\[50000]
loss: 0.63536	cur:[25000]\[50000]
loss: 0.65022	cur:[30000]\[50000]
loss: 0.64655	cur:[35000]\[50000]
loss: 0.64488	cur:[40000]\[50000]
loss: 0.64789	cur:[45000]\[50000]
loss: 0.63249	cur:[50000]\[50000]
epoch:14	avg_epoch_loss1.28191
--------------- Evaluation ---------------
loss: 0.64508	cur:[5000]\[10000]
loss: 0.65131	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:14	avg_running_loss:1.29639
epoch:14	acc:0.54770
*************** epoch:15 ***************
loss: 0.62833	cur:[5000]\[50000]
loss: 0.61298	cur:[10000]\[50000]
loss: 0.61824	cur:[15000]\[50000]
loss: 0.64531	cur:[20000]\[50000]
loss: 0.62466	cur:[25000]\[50000]
loss: 0.61589	cur:[30000]\[50000]
loss: 0.64966	cur:[35000]\[50000]
loss: 0.61706	cur:[40000]\[50000]
loss: 0.62578	cur:[45000]\[50000]
loss: 0.62711	cur:[50000]\[50000]
epoch:15	avg_epoch_loss1.25300
--------------- Evaluation ---------------
loss: 0.63767	cur:[5000]\[10000]
loss: 0.62288	cur:[10000]\[10000]
save model at ./check_point/best.pth
save model at ./check_point/epoch15.pth
epoch:15	avg_running_loss:1.26056
epoch:15	acc:0.54850
*************** epoch:16 ***************
loss: 0.62727	cur:[5000]\[50000]
loss: 0.62718	cur:[10000]\[50000]
loss: 0.61094	cur:[15000]\[50000]
loss: 0.59191	cur:[20000]\[50000]
loss: 0.60780	cur:[25000]\[50000]
loss: 0.61248	cur:[30000]\[50000]
loss: 0.60735	cur:[35000]\[50000]
loss: 0.59495	cur:[40000]\[50000]
loss: 0.60051	cur:[45000]\[50000]
loss: 0.60397	cur:[50000]\[50000]
epoch:16	avg_epoch_loss1.21687
--------------- Evaluation ---------------
loss: 0.66127	cur:[5000]\[10000]
loss: 0.64389	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:16	avg_running_loss:1.30516
epoch:16	acc:0.54540
*************** epoch:17 ***************
loss: 0.59695	cur:[5000]\[50000]
loss: 0.58433	cur:[10000]\[50000]
loss: 0.58396	cur:[15000]\[50000]
loss: 0.60115	cur:[20000]\[50000]
loss: 0.63733	cur:[25000]\[50000]
loss: 0.62033	cur:[30000]\[50000]
loss: 0.62806	cur:[35000]\[50000]
loss: 0.61697	cur:[40000]\[50000]
loss: 0.59871	cur:[45000]\[50000]
loss: 0.58863	cur:[50000]\[50000]
epoch:17	avg_epoch_loss1.21128
--------------- Evaluation ---------------
loss: 0.65682	cur:[5000]\[10000]
loss: 0.66149	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:17	avg_running_loss:1.31831
epoch:17	acc:0.56190
*************** epoch:18 ***************
loss: 0.56985	cur:[5000]\[50000]
loss: 0.58152	cur:[10000]\[50000]
loss: 0.57364	cur:[15000]\[50000]
loss: 0.57540	cur:[20000]\[50000]
loss: 0.56372	cur:[25000]\[50000]
loss: 0.57724	cur:[30000]\[50000]
loss: 0.59421	cur:[35000]\[50000]
loss: 0.57901	cur:[40000]\[50000]
loss: 0.57568	cur:[45000]\[50000]
loss: 0.56818	cur:[50000]\[50000]
epoch:18	avg_epoch_loss1.15169
--------------- Evaluation ---------------
loss: 0.68445	cur:[5000]\[10000]
loss: 0.66945	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:18	avg_running_loss:1.35391
epoch:18	acc:0.53640
*************** epoch:19 ***************
loss: 0.55986	cur:[5000]\[50000]
loss: 0.55839	cur:[10000]\[50000]
loss: 0.59055	cur:[15000]\[50000]
loss: 0.59376	cur:[20000]\[50000]
loss: 0.56227	cur:[25000]\[50000]
loss: 0.56768	cur:[30000]\[50000]
loss: 0.55713	cur:[35000]\[50000]
loss: 0.56142	cur:[40000]\[50000]
loss: 0.54700	cur:[45000]\[50000]
loss: 0.54926	cur:[50000]\[50000]
epoch:19	avg_epoch_loss1.12946
--------------- Evaluation ---------------
loss: 0.57212	cur:[5000]\[10000]
loss: 0.56483	cur:[10000]\[10000]
save model at ./check_point/best.pth
epoch:19	avg_running_loss:1.13695
epoch:19	acc:0.59030
*************** epoch:20 ***************
loss: 0.53633	cur:[5000]\[50000]
loss: 0.53758	cur:[10000]\[50000]
loss: 0.52549	cur:[15000]\[50000]
loss: 0.54511	cur:[20000]\[50000]
loss: 0.53980	cur:[25000]\[50000]
loss: 0.53432	cur:[30000]\[50000]
loss: 0.55291	cur:[35000]\[50000]
loss: 0.54407	cur:[40000]\[50000]
loss: 0.54606	cur:[45000]\[50000]
loss: 0.53158	cur:[50000]\[50000]
epoch:20	avg_epoch_loss1.07865
--------------- Evaluation ---------------
loss: 0.56161	cur:[5000]\[10000]
loss: 0.57596	cur:[10000]\[10000]
save model at ./check_point/best.pth
save model at ./check_point/epoch20.pth
epoch:20	avg_running_loss:1.13757
epoch:20	acc:0.58850
DONE!
acc:	0.5885000000000001
final_lr_0:0.0001
Total Time:1835.5143029689789
